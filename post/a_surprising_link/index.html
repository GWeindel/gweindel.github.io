<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Gabriel Weindel"><meta name=description content="This post describes a surprising result in the preprint &#34;The Decisive Role of Non-Decision Time for Interpreting the Parameters of Decision Making Models&#34; (https://psyarxiv.com/gewb3/)"><link rel=alternate hreflang=en-us href=https://gweindel.github.io/post/a_surprising_link/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.3495fc6150afdd177f1d04fbba9f5e2c.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hud43bd6d9790933c5a17ef89d684a049c_4809_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hud43bd6d9790933c5a17ef89d684a049c_4809_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://gweindel.github.io/post/a_surprising_link/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="GWeindel"><meta property="og:url" content="https://gweindel.github.io/post/a_surprising_link/"><meta property="og:title" content="A surprising link between electrophysiological data in monkeys and a behavioral model in humans | GWeindel"><meta property="og:description" content="This post describes a surprising result in the preprint &#34;The Decisive Role of Non-Decision Time for Interpreting the Parameters of Decision Making Models&#34; (https://psyarxiv.com/gewb3/)"><meta property="og:image" content="https://gweindel.github.io/post/a_surprising_link/featured.png"><meta property="twitter:image" content="https://gweindel.github.io/post/a_surprising_link/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-01-05T14:25:00+00:00"><meta property="article:modified_time" content="2022-01-05T14:25:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://gweindel.github.io/post/a_surprising_link/"},"headline":"A surprising link between electrophysiological data in monkeys and a behavioral model in humans","image":["https://gweindel.github.io/post/a_surprising_link/featured.png"],"datePublished":"2022-01-05T14:25:00Z","dateModified":"2022-01-05T14:25:00Z","author":{"@type":"Person","name":"Gabriel Weindel"},"publisher":{"@type":"Organization","name":"GWeindel","logo":{"@type":"ImageObject","url":"https://gweindel.github.io/media/icon_hud43bd6d9790933c5a17ef89d684a049c_4809_192x192_fill_lanczos_center_3.png"}},"description":"This post describes a surprising result in the preprint \"The Decisive Role of Non-Decision Time for Interpreting the Parameters of Decision Making Models\" (https://psyarxiv.com/gewb3/)"}</script><title>A surprising link between electrophysiological data in monkeys and a behavioral model in humans | GWeindel</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=ce032885b7be41b4b4329fde777f3dda><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>GWeindel</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>GWeindel</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>A surprising link between electrophysiological data in monkeys and a behavioral model in humans</h1><div class=article-metadata><div><span class=author-highlighted>Gabriel Weindel</span></div><span class=article-date>Jan 5, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>7 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/post/>post</a></span></div><div class="btn-links mb-3"></div></div><div class=article-container><div class=article-style><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=test srcset="/post/a_surprising_link/A_surprising_link_files/Macaca_mulatta_hub96d2614ab7aa6aab4b701ff41834989_2043962_9efa200fe549feb87ed357bd72d56997.webp 400w,
/post/a_surprising_link/A_surprising_link_files/Macaca_mulatta_hub96d2614ab7aa6aab4b701ff41834989_2043962_5bece23396c245aeddad95288d9ac53d.webp 760w,
/post/a_surprising_link/A_surprising_link_files/Macaca_mulatta_hub96d2614ab7aa6aab4b701ff41834989_2043962_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/a_surprising_link/A_surprising_link_files/Macaca_mulatta_hub96d2614ab7aa6aab4b701ff41834989_2043962_9efa200fe549feb87ed357bd72d56997.webp width=760 height=137 loading=lazy data-zoomable></div></div></figure></p><h2 id=preamble>Preamble</h2><p>With Thibault Gajdos, Boris Burle and F.-Xavier Alario we recently published a <a href=https://psyarxiv.com/gewb3/ target=_blank rel=noopener>preprint</a> titled &ldquo;The Decisive Role of Non-Decision Time for Interpreting the Parameters of Decision Making Models&rdquo;. As in any article, some choices had to be made and what I am presenting in this post was not the focus of the article (plus the analysis was unplanned and therefore should be taken with a grain of salt), but I still wanted to share this result as I find it extremely interesting.</p><p>In the study reported in the preprint, using experimental manipulations and an electrophysiological measure of response execution, we evaluated the ability of a decision-making model to account for latent cognitive processes in a perceptual decision task.</p><p>In this task we manipulated the amount of force to produce a response, speed accuracy trade-off instructions and finally stimulus contrast. This last manipulation had an interesting property that revealed a surprising pattern ! But let&rsquo;s first review the task of the participants and some of the results from the study.</p><h2 id=the-task>The task</h2><p>Participants had to indicate which of two sinusoidal gratings, left and right of a fixation cross, had the most contrast in it with the corresponding button press (hint : it&rsquo;s on the left on the next image):</p><img align=center src=https://github.com/GWeindel/gweindel.github.io-src/raw/master/content/resources/img/stimulus.png scale=20%><p>Now the contrast manipulation consisted in increasing the overall contrast of both stimuli while maintaining their difference constant :</p><img align=center src=https://github.com/GWeindel/gweindel.github.io-src/raw/master/content/resources/img/stimuli_h.png scale=20%><p>Higher contrast, as on the right of the previous image, yields faster brain response even in the earliest visual sensory systems compared to dimmer stimuli. The nice thing about that manipulation is that, according to the law of Weber-Fechner, the difficulty in comparing the gratings increases with the increase in brightness. This can be seen on the behavioral performance, participants proportion of errors and Reaction Times ($RT$) increase when contrast increases (see the preprint for details of the figure) :</p><img align=center src=https://github.com/GWeindel/gweindel.github.io-src/raw/master/content/resources/img/behavior.png width=700><p>Hence we have here a manipulation where you expect that the time for visual encoding of the stimulus evolves inversly to the time you need to decide. The overall measure of RT cannot show you both effects as they are weighting each other out. Ideally we would like to have a cognitive model of behavior accounting for both dimensions of decision and encoding times but that seems like a hard test for a model.</p><p>In our paper we tested amongst others, whether the well-known Drift Diffusion Model (as implemented by Ratcliff, & Tuerlinckx, 2002) captures simultaneously the decision and visual encoding properties of this manipulation.</p><h2 id=initial-results>Initial results</h2><p>The increase in decision time is obviously captured by the model given the high impact of the increase in contrast on both participants accuracy and $RT$s. More interesting is the fact that the model is also able to capture the decrease in the parameter assumed to relate to visual encoding of the stimulus with the increase in contrast. Let&rsquo;s illustrate it with the actual data from the preprint :</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># I provide the code for each figure just for those who could be interested but you can skip these cells and just look at the outputs</span>
</span></span><span class=line><span class=cl><span class=c1># Following loads the packages and the data from the github of the preprint</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>arviz</span> <span class=k>as</span> <span class=nn>az</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>style</span><span class=o>.</span><span class=n>use</span><span class=p>(</span><span class=s1>&#39;seaborn-ticks&#39;</span><span class=p>)</span><span class=c1>#Plotting theme</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>PMTstats</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://github.com/GWeindel/The-Decisive-Role-of-Non-Decision-Time/raw/main/DDM/PMT_M13_stats.csv&#39;</span><span class=p>,</span> <span class=n>index_col</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=c1>## Recovering DDM estimates from fit</span>
</span></span><span class=line><span class=cl><span class=n>PMTtraces</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://github.com/GWeindel/The-Decisive-Role-of-Non-Decision-Time/raw/main/DDM/PMT_M13_traces.csv&#39;</span><span class=p>,</span> <span class=n>index_col</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=c1>## See the preprint for the details</span>
</span></span></code></pre></div><p>The following figure illustrates this observation, the parameter for encoding time ($T_{e}$ y-axis) is indeed decreasing with contrast increase (x-axis).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span><span class=mi>4</span><span class=p>),</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span><span class=c1>#Graphical parameters</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>PMTstats</span><span class=p>[</span><span class=n>PMTstats</span><span class=o>.</span><span class=n>index</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>contains</span><span class=p>(</span><span class=s2>&#34;t\(low.Speed&#34;</span><span class=p>)][</span><span class=s2>&#34;mean&#34;</span><span class=p>]</span><span class=c1>#Taking a subset (the best) of the estimates from the preprints</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting these points</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>errorbar</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>23</span><span class=p>,</span><span class=mi>93</span><span class=p>,</span><span class=mi>6</span><span class=p>),</span> <span class=n>m</span><span class=o>*</span><span class=mi>1000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>             <span class=n>ls</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s2>&#34;o&#34;</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&#34;indianred&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Other graphical parameters</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;$T_</span><span class=si>{e}</span><span class=s1>$ Encoding Time parameter (ms)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Contrast (%)&#39;</span><span class=p>);</span>
</span></span></code></pre></div><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/a_surprising_link/A_surprising_link_files/A_surprising_link_7_0_hu323d6d132363869313078ffa8a9f02fb_20379_2dc54ad5fd8565767fdffa37f765b5f0.webp 400w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_7_0_hu323d6d132363869313078ffa8a9f02fb_20379_2ce30f4b0610547b80c232483f55f379.webp 760w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_7_0_hu323d6d132363869313078ffa8a9f02fb_20379_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/a_surprising_link/A_surprising_link_files/A_surprising_link_7_0_hu323d6d132363869313078ffa8a9f02fb_20379_2dc54ad5fd8565767fdffa37f765b5f0.webp width=698 height=549 loading=lazy data-zoomable></div></div></figure></p><p><strong>Great news</strong>, a model like the drift diffusion model, using only RTs and choices, is able to show that visual encoding time is decreasing even though difficulty is increasing the $RT$s (although that is only true under some conditions and there is some uncertainty associated but see the preprint).</p><h2 id=a-surprising-link>A surprising link</h2><p>Now one thing is particularly interesting. I started to talk with scientists with more expertise in visual processing about the experiment. At that occasion I was redirected to an article written in 2012 by Reynaud, Masson and Chavane (<a href=https://doi.org/10.1523/JNEUROSCI.1618-12.2012%29 target=_blank rel=noopener>https://doi.org/10.1523/JNEUROSCI.1618-12.2012)</a>. In their article the authors analyze, using voltage-sensitive dye, the temporal dynamics of visual neuron in the primary visual cortex, V1, of two monkeys (<em>Macaca mulata</em>) in response to a visual grating similar to the one we used. In one of their analysis (see Figure 5B. of their article) they report measurement of onset of activity in V1 neurons in response to different contrasts of the grating. Using the Naka-Rushton equation, a well known luminance-response function, in its inverted version (Barthelemy, Fleuriet & Masson, 2010) they provide a description of the onset of activity in V1 in response to different contrast levels.</p><p>Now things are getting even more interesting. One test we could do is : does the data from these monkeys relate to our estimate of visual encoding time in human using only a behavioral model (and some physiological information about response execution) ?</p><p>That would be wild but we tested it anyway, especially because the lead author of the paper on V1 activity in monkeys guessed that we should have a non-linear relationship which is indeed apparent in the previous Figure. The only thing we had to do was to recover the parameters of the inverted Naka-Rushton they obtained and generate predictions for the contrast levels we used in our study. Then we can just compare their predictions and our estimate of visual encoding in humans.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>Naka_Rushton</span><span class=p>(</span><span class=n>par</span><span class=p>,</span><span class=n>contrast</span><span class=p>,</span><span class=n>maxAmp</span><span class=p>,</span> <span class=n>maxShift</span><span class=p>):</span><span class=c1>#Inverted Naka-rushton equation from Barthélemy, Fleuriet and Masson 2010</span>
</span></span><span class=line><span class=cl>    <span class=n>c50</span><span class=p>,</span> <span class=n>slope</span> <span class=o>=</span> <span class=n>par</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span><span class=n>par</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>maxAmp</span> <span class=o>+</span> <span class=n>maxShift</span> <span class=o>*</span> <span class=p>((</span><span class=n>contrast</span><span class=o>**</span><span class=n>slope</span><span class=p>)</span><span class=o>/</span><span class=p>((</span><span class=n>c50</span><span class=o>**</span><span class=n>slope</span><span class=p>)</span><span class=o>+</span><span class=p>(</span><span class=n>contrast</span><span class=o>**</span><span class=n>slope</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Hereby we obtain the prediction of onset of neurons in V1, for the contrast level we used, with the parameters we recovered from their article </span>
</span></span><span class=line><span class=cl><span class=c1># To see how we recovered the parameters see https://github.com/GWeindel/The-Decisive-Role-of-Non-Decision-Time/blob/main/3-DDM_analysis.ipynb</span>
</span></span><span class=line><span class=cl><span class=n>predicted_V1</span> <span class=o>=</span> <span class=n>Naka_Rushton</span><span class=p>(</span><span class=n>par</span><span class=o>=</span><span class=p>[</span><span class=mf>11.42</span><span class=p>,</span><span class=o>-</span><span class=mf>1.54</span><span class=p>]</span> <span class=p>,</span> <span class=n>maxAmp</span> <span class=o>=</span> <span class=mf>36.99</span><span class=p>,</span> <span class=n>maxShift</span><span class=o>=</span><span class=mf>51.66</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                <span class=n>contrast</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>23</span><span class=p>,</span><span class=mi>93</span><span class=p>,</span><span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span><span class=mi>4</span><span class=p>),</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>23</span><span class=p>,</span><span class=mf>93.1</span><span class=p>,</span><span class=mi>100</span><span class=p>),</span> <span class=n>predicted_V1</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;green&#34;</span><span class=p>)</span><span class=c1>#Plotting predictions</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Contrast levels (%)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;V1 predicted neuron response time (ms)&#39;</span><span class=p>);</span>
</span></span></code></pre></div><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/a_surprising_link/A_surprising_link_files/A_surprising_link_11_0_hu0c38dbf6c3230269a62eee5240498099_30912_cb60348697b067f40e7368cabb1d3976.webp 400w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_11_0_hu0c38dbf6c3230269a62eee5240498099_30912_58c703e2c7d9f2ea368cd81f9cd3ca75.webp 760w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_11_0_hu0c38dbf6c3230269a62eee5240498099_30912_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/a_surprising_link/A_surprising_link_files/A_surprising_link_11_0_hu0c38dbf6c3230269a62eee5240498099_30912_cb60348697b067f40e7368cabb1d3976.webp width=681 height=549 loading=lazy data-zoomable></div></div></figure></p><p>Damned that looks close !! What if we superpose both ? Well we could do that simply by displaying both, but obviously V1 neurons respond really fast and visual processing goes on beyond the first response latency, therefore we need to be on the same scale. We did this be simply subtracting the mean of each variable (human behavioral model vs V1 measurment) to each data point from the same variable ($x_i - \overline{x}$). Doing so we only keep the relative difference of each contrast point to the mean of each variable.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span><span class=mi>4</span><span class=p>),</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># plotting the centered predictions</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>23</span><span class=p>,</span><span class=mi>93</span><span class=p>,</span><span class=mi>100</span><span class=p>),</span> <span class=n>predicted_V1</span><span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>predicted_V1</span><span class=p>),</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;green&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>r</span><span class=s1>&#39;V1 predictions&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Recovering the estimates of the behavioral model for humans</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>PMTstats</span><span class=p>[</span><span class=n>PMTstats</span><span class=o>.</span><span class=n>index</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>contains</span><span class=p>(</span><span class=s2>&#34;t\(low.Speed&#34;</span><span class=p>)][</span><span class=s2>&#34;mean&#34;</span><span class=p>]</span><span class=c1># I only use mean, see preprint for full uncertainty representation</span>
</span></span><span class=line><span class=cl><span class=n>m_scaled</span> <span class=o>=</span> <span class=n>m</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>m</span><span class=p>)</span> <span class=c1># centering</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># plotting the centered estimates</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>errorbar</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>23</span><span class=p>,</span><span class=mi>93</span><span class=p>,</span><span class=mi>6</span><span class=p>),</span> <span class=n>m_scaled</span><span class=o>*</span><span class=mi>1000</span><span class=p>,</span> <span class=c1>#*1000 to convert to milliseconds</span>
</span></span><span class=line><span class=cl>             <span class=n>ls</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s2>&#34;o&#34;</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&#34;indianred&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;$T_</span><span class=si>{e}</span><span class=s1>$ estimates&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Paramètres graphiques</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>frameon</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;Centered Time (ms)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Contrast&#39;</span><span class=p>);</span>
</span></span></code></pre></div><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/a_surprising_link/A_surprising_link_files/A_surprising_link_13_0_huba15d2f579518c5805200bc586c4c11d_31798_0933440de5036417aa2f35b1fe66b3ba.webp 400w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_13_0_huba15d2f579518c5805200bc586c4c11d_31798_84f38307d544dab836709ce0ef759d0b.webp 760w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_13_0_huba15d2f579518c5805200bc586c4c11d_31798_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/a_surprising_link/A_surprising_link_files/A_surprising_link_13_0_huba15d2f579518c5805200bc586c4c11d_31798_0933440de5036417aa2f35b1fe66b3ba.webp width=682 height=549 loading=lazy data-zoomable></div></div></figure></p><p>Wow&mldr; That is impressive !</p><p>A behavioral model (+ physiology of response execution) fitted on data from a decision making task performed by humans is capturing visual encoding latencies that very closely match actual measurment of primary visual neurons in monkeys ! It is important to note that <strong>no fit occurred</strong> between V1 predictions and the estimated encoding time, we just centered each variable !</p><p>Now the story doesn&rsquo;t end here This congruence for example only happens when we particularly stress participants to respond as fast as possible. When we switch to an accuracy-focused condition, things do really not look the same :</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span><span class=mi>4</span><span class=p>),</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># plotting the centered predictions</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>23</span><span class=p>,</span><span class=mi>93</span><span class=p>,</span><span class=mi>100</span><span class=p>),</span> <span class=n>predicted_V1</span><span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>predicted_V1</span><span class=p>),</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;green&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>r</span><span class=s1>&#39;V1 predictions&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Recovering the estimates of the behavioral model for humans</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>PMTstats</span><span class=p>[</span><span class=n>PMTstats</span><span class=o>.</span><span class=n>index</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>contains</span><span class=p>(</span><span class=s2>&#34;t\(low.Accuracy&#34;</span><span class=p>)][</span><span class=s2>&#34;mean&#34;</span><span class=p>]</span><span class=c1># I only use mean, see preprint for full uncertainty representation</span>
</span></span><span class=line><span class=cl><span class=n>m_scaled</span> <span class=o>=</span> <span class=n>m</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>m</span><span class=p>)</span> <span class=c1># centering</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># plotting the centered estimates</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>errorbar</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>23</span><span class=p>,</span><span class=mi>93</span><span class=p>,</span><span class=mi>6</span><span class=p>),</span> <span class=n>m_scaled</span><span class=o>*</span><span class=mi>1000</span><span class=p>,</span> <span class=c1>#*1000 to convert to milliseconds</span>
</span></span><span class=line><span class=cl>             <span class=n>ls</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s2>&#34;o&#34;</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&#34;indianred&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;$T_</span><span class=si>{e}</span><span class=s1>$ estimates&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Paramètres graphiques</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>frameon</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;Centered Time (ms)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Contrast&#39;</span><span class=p>);</span>
</span></span></code></pre></div><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/a_surprising_link/A_surprising_link_files/A_surprising_link_15_0_hu4ff691015515588791d55264de34041c_31966_90edd8ae339b61660d19a3910e191e5f.webp 400w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_15_0_hu4ff691015515588791d55264de34041c_31966_0b563a6a5d334e5422f61654584a26f7.webp 760w,
/post/a_surprising_link/A_surprising_link_files/A_surprising_link_15_0_hu4ff691015515588791d55264de34041c_31966_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/a_surprising_link/A_surprising_link_files/A_surprising_link_15_0_hu4ff691015515588791d55264de34041c_31966_90edd8ae339b61660d19a3910e191e5f.webp width=682 height=549 loading=lazy data-zoomable></div></div></figure></p><p>But still, the fact that physiological measurment of primary visual cortex neurons from another specie predicts that well the estimates of a behavioral model of decision making in humans is a really impressive performance !</p><h3 id=bibliography>Bibliography</h3><ul><li>Barthélemy, F. V., Fleuriet, J., & Masson, G. S. (2010). Temporal dynamics of 2D motion integration for ocular following in macaque monkeys. Journal of neurophysiology, 103(3), 1275-1282.</li><li>Ratcliff, R., & Tuerlinckx, F. (2002). Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability. Psychonomic bulletin & review, 9(3), 438-481.</li><li>Reynaud, A., Masson, G. S., & Chavane, F. (2012). Dynamics of local input normalization result from balanced short-and long-range intracortical interactions in area V1. Journal of neuroscience, 32(36), 12558-12569.</li></ul></div><div class=article-tags><a class="badge badge-light" href=/tag/modeling/>modeling</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://gweindel.github.io/post/a_surprising_link/&text=A%20surprising%20link%20between%20electrophysiological%20data%20in%20monkeys%20and%20a%20behavioral%20model%20in%20humans" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://gweindel.github.io/post/a_surprising_link/&t=A%20surprising%20link%20between%20electrophysiological%20data%20in%20monkeys%20and%20a%20behavioral%20model%20in%20humans" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=A%20surprising%20link%20between%20electrophysiological%20data%20in%20monkeys%20and%20a%20behavioral%20model%20in%20humans&body=https://gweindel.github.io/post/a_surprising_link/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://gweindel.github.io/post/a_surprising_link/&title=A%20surprising%20link%20between%20electrophysiological%20data%20in%20monkeys%20and%20a%20behavioral%20model%20in%20humans" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=A%20surprising%20link%20between%20electrophysiological%20data%20in%20monkeys%20and%20a%20behavioral%20model%20in%20humans%20https://gweindel.github.io/post/a_surprising_link/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://gweindel.github.io/post/a_surprising_link/&title=A%20surprising%20link%20between%20electrophysiological%20data%20in%20monkeys%20and%20a%20behavioral%20model%20in%20humans" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://gweindel.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu35a75dca4ececef2d7cdbdcaef3615bb_694501_270x270_fill_q75_lanczos_center.jpg alt="Gabriel Weindel"></a><div class=media-body><h5 class=card-title><a href=https://gweindel.github.io/>Gabriel Weindel</a></h5><h6 class=card-subtitle>Post-doctoral researcher</h6><p class=card-text>My research interests is mental chronometry, the time taken by information processing units in our brain. To study these times I rely on cognitive models of behavior that I combine with neural data such as electro-encephalography.</p><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/GWeindel target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=o_fW4gIAAAAJ&hl=fr&oi=ao" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/Gweindel target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/gabriel-weindel target=_blank rel=noopener><i class="fabgcushen fabgcushen-linkedin"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.54dd6e4d8f2e4b1d098381b57f18dd83.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>